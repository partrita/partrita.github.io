{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 0. 시작하기전에\n",
    "\n",
    "[edwith](http://www.edwith.org/aipython/joinLectures/14365)에서 최성철교수님의 강의를 듣고 노트정리를 한 내용입니다. 따라서 사용한 모든 코드와 내용의 저작권은 최성철 교수님에게 있습니다. 보다 자세한 내용은 위의 링크에서 강의를 수강하시기 바랍니다. \n",
    "\n",
    "\n",
    "# 1. 교육 환경\n",
    "모든 라이브러리는 `Anaconda`를 이용해 설치 하였습니다. 사용한 라이브러리의 목록은 다음과 같습니다.\n",
    "\n",
    "- Jupyter\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Scikit learn\n",
    "\n",
    "\n",
    "# 2. 파이썬 코드 스타일\n",
    "파이썬을 활용하여 효율적으로 코드를 표현하는 기법을 배워 봅니다. 예를 들면 다음과 같은 리스트안의 내용을 한줄로 출력하는 코드가 있습니다.\n",
    "\n",
    "```python\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "result = \"\"\n",
    "for s in colors:\n",
    "    result += s\n",
    "```\n",
    "\n",
    "위의 코드를 파이썬스럽게 만들면, 아래와 같습니다.\n",
    "\n",
    "```python\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "result = \"\".join(colors)\n",
    "```\n",
    "이렇게 하면 간결하고 가독성이 좋아집니다. \n",
    "\n",
    "## 2.1. Split & Join\n",
    "\n",
    "Split & Join 을 사용하여 String Type 의 값을 List 형태로 변환하고, List Type의 값을 String Type 의\n",
    "값으로 변환해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈칸을 기준으로 문자열 나누기\n",
    "items = 'zero one two three'.split() \n",
    "# 쉼표를 기준으로 문자열 나누기\n",
    "items = 'zero,one,two,three'.split()\n",
    "# \".\"을 기준으로 문자열 나누고 unpacking\n",
    "example = 'cs50.gachon.edu'\n",
    "subdomain, domain, tld = example.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. List Comprehension\n",
    "많이 사용되는 기법 중에 하나인 List Comprehension 을 사용해봅니다. 비교를 하기 위해 먼저, `for loop` + `append()` 사용한 코드를 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(10):\n",
    "    result.append(i)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list Comprehension 사용하면 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "result = [i for i in range(10)]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Enumerate & Zip\n",
    "리스트의 값을 추출할때 방법으로 이용되는 `enumerate` 와 두개의 `list` 값을 병렬적으로 추출할 수 있는 `zip` 모듈을 알아 봅니다.\n",
    "\n",
    "### 2.3.1. Enumerate\n",
    "\n",
    "- 리스트의 값을 index 번호와 함께 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "my_list = ['a','b','c']\n",
    "for i, j in enumerate(my_list):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b'), (2, 'c')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(my_list)) # 리스트에 있는 index와 값을 unpacking 후 다시 리스트로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Zip\n",
    "두 개의 리스트 값을 병렬적으로 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a1', 'b1') ('a2', 'b2') ('a3', 'b3') "
     ]
    }
   ],
   "source": [
    "list_a = ['a1','a2','a3']\n",
    "list_b = ['b1','b2','b3']\n",
    "for i in zip(list_a,list_b):\n",
    "    print(i, end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4. Lambda & Map & Reduce\n",
    "\n",
    "- 함수처럼 사용가능한 `Lambda`\n",
    "- Sequence 자료형의 데이터에서 함수를 적용하는 방법인 `Map` 과 `Reduce` 함수\n",
    "\n",
    "### 2.4.1. Lambda\n",
    "\n",
    "다음과 같은 코드를 `Lambda`로 보다 간략하게 쓸 수 있습니다.\n",
    "\n",
    "```python\n",
    "def f(x, y):\n",
    "    return x + y\n",
    "print(f(1, 4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Lambda 사용\n",
    "f = lambda x, y: x + y\n",
    "print(f(1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 제 개인적 의견으로는 `Lambda`는 가독성을 나쁘게 해서, 좋아하지 않습니다. \n",
    " \n",
    "### 2.4.2. Map & Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "#python 3에는 list를 붙여야 합니다.\n",
    "ex = [1,2,3,4,5]\n",
    "print(list(map(lambda x: x+x, ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Reduce\n",
    "from functools import reduce\n",
    "print(reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 배운것으로 팩토리얼을 구현해 봅니다.\n",
    "def factorial(n):\n",
    "    return reduce(lambda x, y: x*y, range(1, n+1))\n",
    "factorial(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Asterisk\n",
    "곱셈, 제곱연산, 가변인자 활용 등 여러 부분에서 다양하게 사용되는 Asterisk(*) 의 사용법을 알아봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2, 3, 4, 5, 6)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "def asterisk_test(a, *args):\n",
    "    print(a, args)\n",
    "    print(type(args))\n",
    "\n",
    "asterisk_test(1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "def asterisk_test(a, **kargs):\n",
    "    print(a, kargs)\n",
    "    print(type(kargs))\n",
    "\n",
    "asterisk_test(1, b=2, c=3, d=4, e=5, f=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2, 3, 4, 5, 6)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "def asterisk_test(a, *args):\n",
    "    print(a, args[0])\n",
    "    print(type(args))\n",
    "\n",
    "asterisk_test(1, (2, 3, 4, 5, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "def asterisk_test(a, args):\n",
    "    print(a, *args)\n",
    "    print(type(args))\n",
    "\n",
    "asterisk_test(1, (2,3,4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] [3, 4] [5, 6]\n"
     ]
    }
   ],
   "source": [
    "a, b, c = ([1, 2], [3, 4], [5, 6])\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] [3, 4] [5, 6]\n"
     ]
    }
   ],
   "source": [
    "data = ([1, 2], [3, 4], [5, 6])\n",
    "print(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for data in zip(*([1, 2], [3, 4], [5, 6])):\n",
    "    print(sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3 2 1 56\n"
     ]
    }
   ],
   "source": [
    "def asterisk_test(a, b, c, d, e=0):\n",
    "    print(a, b, c, d, e)\n",
    "\n",
    "data = {\"d\":1 , \"c\":2, \"b\":3, \"e\":56 }\n",
    "asterisk_test(10, **data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Collections\n",
    "tuple, dict 에 대한 확장 데이터 구조를 제공하는 Collections 안에 포함된 모듈을 이용하여 Data Sturcture 의 기본\n",
    "개념을 이해하고 사용하는 방법을 알아봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()                           # a new, empty counter\n",
    "c = Counter('gallahad')                 # a new counter from an iterable\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 선형대수(Linear algebra) 문제 풀어보기\n",
    "파이썬으로 선형대수를 다루는 방법을 알아봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2, 3), (2, 3, 5)]\n"
     ]
    }
   ],
   "source": [
    "u = [2, 2]\n",
    "v = [2, 3]\n",
    "z = [3, 5]\n",
    "\n",
    "result = [t for t in zip(u, v, z)]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 14], [10, 12]]\n"
     ]
    }
   ],
   "source": [
    "matrix_a = [[3, 6], [4, 5]]\n",
    "matrix_b = [[5, 8], [6, 7]]\n",
    "result = [[sum(row) for row in zip(*t)]\n",
    "          for t in zip(matrix_a, matrix_b)]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4], [2, 5], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "matrix_a = [[1, 2, 3], [4, 5, 6]]\n",
    "result = [[element for element in t] for t in zip(*matrix_a)]\n",
    "\n",
    "[t for t in zip(*matrix_a)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 8], [5, 6]]\n"
     ]
    }
   ],
   "source": [
    "matrix_a = [[1, 1, 2], [2, 1, 1]]\n",
    "matrix_b = [[1, 1], [2, 1], [1, 3]]\n",
    "result = [[sum(a * b for a, b in zip(row_a, column_b))\n",
    "          for column_b in zip(*matrix_b)] for row_a in matrix_a]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 선형대수 과제에 대한 코드입니다.  vector와 matrix의 기초적인 연산을 수행하는 12개의 함수를 작성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Problem #1 - vector_size_check\n",
    "def vector_size_check(*vector_variables):\n",
    "    return all(len(vector_variables[0]) == len(i) for i in vector_variables)\n",
    "\n",
    "# 실행결과\n",
    "print(vector_size_check([1,2,3], [2,3,4], [5,6,7])) # Expected value: True\n",
    "print(vector_size_check([1, 3, 4], [4], [6,7])) # Expected value: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 14]\n",
      "[15, 16]\n"
     ]
    }
   ],
   "source": [
    "# Problem #2 - vector_addition \n",
    "def vector_addition(*vector_variables):\n",
    "    return [sum(i) for i in zip(*vector_variables)]\n",
    "\n",
    "# 실행결과\n",
    "print(vector_addition([1, 3], [2, 4], [6, 7])) # Expected value: [9, 14]\n",
    "print(vector_addition([1, 5], [10, 4], [4, 7])) # Expected value: [15, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 김동혁 <hyukster9@gmail.com> 님이 위의 코드의 오류를 정정해주셨습니다. 감사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1]\n",
      "[-13, -6]\n"
     ]
    }
   ],
   "source": [
    "# Problem #3 - vector_subtraction\n",
    "def vector_subtraction(*vector_variables):\n",
    "    if vector_size_check(*vector_variables) == False:\n",
    "        raise ArithmeticError\n",
    "    return [i[0]*2 - sum(i) for i in zip(*vector_variables)]\n",
    "\n",
    "# 실행결과\n",
    "print(vector_subtraction([1, 3], [2, 4])) # Expected value: [-1, -1]\n",
    "print(vector_subtraction([1, 5], [10, 4], [4, 7])) # Expected value: [-13, -6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 15]\n",
      "[6, 6]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "# Problem #4 - scalar_vector_product (one line code available)\n",
    "def scalar_vector_product(alpha, vector_variable):\n",
    "    return [alpha*i for i in vector_variable]\n",
    "\n",
    "# 실행결과\n",
    "print (scalar_vector_product(5,[1,2,3])) # Expected value: [5, 10, 15]\n",
    "print (scalar_vector_product(3,[2,2])) # Expected value: [6, 6]\n",
    "print (scalar_vector_product(4,[1])) # Expected value: [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 머신러닝의 개요\n",
    "이번 장에서는 `Machine Learning` 에서 사용하는 용어와 개념에 대해서 공부합니다.\n",
    "\n",
    "- Model: 예측을 위한 수학 공식, 함수 \n",
    "    - e.g. 1차 방정식, 확률분포, condition rule\n",
    "- Algorithms: 어떠한 문제를 풀기 위한 과정, Model을 생성하기 위한 과정\n",
    "- Feature : 머신러닝에서 데이터의 특징을 나타내는 변수\n",
    "    - 연속형(continuous feature): 온도, 속도, 일반적인 실수값\n",
    "    - 이산형(discrete feature): 성별, 등수\n",
    "\n",
    "# 5. 데이터 다루기\n",
    "\n",
    "## 5.1. Numpy 사용법\n",
    "과학 계산용 패키지인 `numpy` 의 여러 특징과 기능, 코드를 작성하는 방법 등을 배웁니다.  먼저, 라이브러리를 불러오고 `array`를 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f9f53ae78660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# array의 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# array의 생성\n",
    "test_matrix = np.array([[1,2,3,4], [1,2,5,8]])\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`array`의 모양 확인하는 방법은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Array 모양 바꾸기\n",
    "`array`의 모양을 평평하게(1D array) 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_matrix).reshape(8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 방법으로 `flatten()`를 사용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_matrix).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. slicing\n",
    "array의 특정 영역만 선택하는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix[:,1] # 2번째 column 부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix[1,:] # 2번째 row 부터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. Concatenate\n",
    "각각의 `array`를 하나로 합쳐줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3]])\n",
    "b = np.array([[2, 3, 4]])\n",
    "np.concatenate( (a,b) ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate( (a,b) ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4. 데이터 타입 바꿔주기\n",
    "데이터 타입을 확인하고 원하는 것으로 바꿔주는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float 형으로 변경해줍니다.\n",
    "test_matrix_float = test_matrix.astype(float)\n",
    "test_matrix_float.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Pandas\n",
    "파이썬의 **엑셀**이라 불리는 `Pandas`의 여러 기능과 사용하는 방법을 설명합니다. `pandas`는 `series`와 `dataframe`이라는 형태로 데이터를 처리합니다. \n",
    "- series : 1차원 배열\n",
    "- dataframe : 2차원 배열\n",
    "\n",
    "\n",
    "### 5.2.1. 데이터 불러오기\n",
    "`pandas`로 외부데이터를 불러오는 방법입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = 'https://www.shanelynn.ie/wp-content/uploads/2015/06/phone_data.csv'\n",
    "df = pd.read_csv(data_url, index_col = 0)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 829개의 데이터를 가져왔습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. 데이터 선택하기\n",
    "#### Index 정보로 데이터 선택법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3] # 위에서 3번째까지의 데이터 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 정보로 데이터 선택법\n",
    "\n",
    "**duration**과 **network** column의 값만 선택하고, 위에서부터 3개의 값만 선택해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['duration','network']][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3. 데이터 주무르기\n",
    "\n",
    "`dateutil`라이브러리를 사용하면 날짜데이터를 쉽게 파싱할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "달별 총 통화량을 계산해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month')['duration'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피봇테이블 기능을 이용해서도 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(['duration'], index = df.month, aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4.  NA 값 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'first_name': ['Jason', np.nan, 'Tina', 'Jake', 'Amy'],\n",
    "        'last_name': ['Miller', np.nan, 'Ali', 'Milner', 'Cooze'],\n",
    "        'age': [42, np.nan, 36, 24, 73],\n",
    "        'sex': ['m', np.nan, 'f', 'm', 'f'],\n",
    "        'preTestScore': [4, np.nan, np.nan, 2, 3],\n",
    "        'postTestScore': [25, np.nan, np.nan, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA 값이 하나라도 있는 데이터는 지우기\n",
    "df_no_missing = df.dropna(axis=0, thresh=6)\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 값이 NA인 데이터 지우기\n",
    "df_cleaned = df.dropna(how='all')\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA값을 0으로 바꾸기\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5. One-hot encoding\n",
    "`get_dummies()`함수를 이용해 one-hot encoding을 해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성별 컬럼을 one-hot-encoding 해보겠습니다.\n",
    "pd.concat([df,pd.get_dummies(df['sex'], prefix='sex')], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.6. feature_scaling\n",
    "`pandas`로 구현하는 방법도 있지만, `sklearn`의 `preprocessing` 기능을 사용하는것이 보다 편합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df = df.fillna(0) # NA값을 제거하기 위해\n",
    "minmax_scaler = preprocessing.MinMaxScaler().fit(df[['preTestScore','postTestScore']])\n",
    "df[['preTestScore','postTestScore']] = minmax_scaler.transform(df[['preTestScore','postTestScore']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preTestScore, postTestScore의 값이 0~1사이의 값으로 정규화(normalization)되었습니다.\n",
    "\n",
    "# 6. Numpy 과제풀이\n",
    "숙제로 나온 과제 일부를 풀어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_matrix = np.array([[1,2,3,4], [1,2,5,8]])\n",
    "\n",
    "def change_shape_of_ndarray(X, n_row):\n",
    "    if n_row == 1:\n",
    "        return X.flatten()\n",
    "    else:\n",
    "        return X.reshape(n_row, -1)\n",
    "\n",
    "change_shape_of_ndarray(test_matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = np.array([1,2,3,4])\n",
    "\n",
    "def save_ndarray(X, filename=\"test.npy\"):\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(X, f)\n",
    "\n",
    "def boolean_index(X, condition):\n",
    "    condition = eval(str('X') + condition )\n",
    "    return np.where(condition)\n",
    "\n",
    "boolean_index(test_matrix, '>2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = np.array([1,2,3,4])\n",
    "def find_nearest_value(X, target_value):\n",
    "    return X[np.argmin(np.abs(X - target_value))]\n",
    "\n",
    "find_nearest_value(test_matrix, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = np.array([1,2,3,4])\n",
    "def get_n_largest_values(X, n):\n",
    "    return X[np.argsort(X[::-1])[:n]]\n",
    "get_n_largest_values(test_matrix, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 선형회귀(Linear regression)\n",
    "\n",
    "> 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법 --wikipedia\n",
    "\n",
    "앞으로 `scikit-learn` 라이브러리를 가지고 머신러닝을 진행할 것입니다. 머신러닝을 분류하는 기준은 여러가지가 있지만, 여기서는 다음 4가지로 분류 합니다.\n",
    "\n",
    "1. Gradient descent based learning \n",
    "2. Probability theory based learning\n",
    "3. Information theory based learning\n",
    "4. Distance similarity based learning\n",
    "\n",
    "머신러닝의 목적은 아래 같습니다,\n",
    "- 실제 값과 학습된 모델 예측치의 오차를 최소화\n",
    "- 모델의 최적 weight값 찾기\n",
    "\n",
    "\n",
    "선형회귀에서 오차를 측정하는 방법으로 **Squared Error**를 사용합니다. 따라서 Squared Error를 최소화 할 수 있는 weight값을 찾는 것이 목표가 됩니다.\n",
    "\n",
    "## 7.1. Cost function\n",
    "\n",
    "**cost function** 은 실제 값과 예측된 값의 차이를 나타낸 수식입니다.  cost functon의 최소값을 찾으면 최적 weights값을 찾을 수 있습니다. 아래 두가지 방법을 알아 보겠습니다.  \n",
    "- normal equation\n",
    "- gradient descent\n",
    "\n",
    "## 7.2. Normal equation\n",
    "특징은 다음과 같습니다.\n",
    "- X<sup>T</sup>X의 역행렬이 존재할 때 사용\n",
    "- hyper parameter가 없음\n",
    "- Feature가 많으면 계산 속도가 느려짐\n",
    "\n",
    "#### Hyper parameter\n",
    "사용자가 임의로 정해줘야하는 변수입니다. 예를 들면 학습률(Learning rate)가 있습니다.  \n",
    "- 학습률이 너무 낮게 설정: 학습 시간이 오래 걸림\n",
    "- 학습률을 너무 높게 설정: 수렴하지 못하는 경우가 생김\n",
    "\n",
    "아래는 실습 코드입니다.\n",
    "\n",
    "### 7.2.1 실습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#  LOAD DATASET - simple variable\n",
    "df_train = pd.read_csv(\"./data/normal_eq_train.csv\")\n",
    "df_test  =  pd.read_csv(\"./data/normal_eq_test.csv\")\n",
    "# df_test.head()\n",
    "X_train = df_train[\"x\"].values.reshape(-1,1)\n",
    "X_test = df_test[\"x\"].values.reshape(-1,1)\n",
    "y_train = df_train[\"y\"].values\n",
    "y_test = df_test[\"y\"].values\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 시각화하면 아래 그림과 같습니다. 명확하게 선형성(Linearity)이 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train,y_train, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 만들기\n",
    "\n",
    "`sklearn`의 선형회귀 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr = linear_model.LinearRegression(normalize=False) # false 이유?\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  예측하기 하고 시각화하기\n",
    "테스트 값(x)을 넣어 예측 값(y)를 구하고 시각화 해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: {:02.3f} '.format(lr.coef_[0]))\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: {:02.3f}\".format(mean_squared_error(y_test, y_pred)))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: {:02.3f}'.format(r2_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  alpha = 0.3)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Gradient Descent\n",
    "\n",
    "**Gradient Descent** 알고리즘으로 **Linear Regression** 을 구하는 방법에 대해 공부합니다\n",
    "\n",
    "\n",
    "### 7.3.1. Linear regression with Gradient Descent\n",
    "- 임의의 theta0, theta1 값으로 초기화\n",
    "- Cost function 이 최소화 될 때까지 학습\n",
    "- Learning rate, Iteration 횟수 등 hyper parameter 필요\n",
    "- Feature가 많으면 Normal equation에 비해 상대적으로 빠름\n",
    "- 다만, 최적값에 수렴하지 않을 수도 있음\n",
    "\n",
    "#### 예시 코드\n",
    "사용한 데이터는 [스웨덴 자동차 보험](https://www.kaggle.com/floser/swedish-motor-insurance/version/1#) 입니다. x를 횟수(Claims) 값을 두고 y의 값은 지불(Payment)열로 지정해서 보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "df = pd.read_csv(\"./data/SwedishMotorInsurance.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = number of claims\n",
    "# Y = total payment for all the claims in thousands of Swedish Kronor\n",
    "raw_X = df['Claims'].values.reshape(-1, 1)\n",
    "y = df[\"Payment\"].values/10000\n",
    "plt.plot(raw_X,y, 'o', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((len(raw_X),1))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate( (np.ones((len(raw_X),1)), raw_X ), axis=1)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.normal((0,4))  # 초기 weight값을 임의로 정해줍니다. \n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.dot(X, w)\n",
    "plt.plot(raw_X,y,\"o\", alpha=0.5)\n",
    "plt.plot(raw_X,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS AND COST FUNCTION\n",
    "def hypothesis_function(X, theta):\n",
    "    return X.dot(theta)\n",
    "def cost_function(h, y):\n",
    "    return (1/(2*len(y))) * np.sum((h-y)**2)\n",
    "h = hypothesis_function(X,w)\n",
    "cost_function(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT\n",
    "def gradient_descent(X, y, w, alpha, iterations):\n",
    "    theta = w\n",
    "    m = len(y) \n",
    "    theta_list = [theta.tolist()]\n",
    "    cost = cost_function(hypothesis_function(X, theta), y)\n",
    "    cost_list = [cost]\n",
    "    for i in range(iterations):\n",
    "        t0 = theta[0] - (alpha / m) * np.sum(np.dot(X, theta) - y)\n",
    "        t1 = theta[1] - (alpha / m) * np.sum((np.dot(X, theta) - y) * X[:,1])\n",
    "        theta = np.array([t0, t1])        \n",
    "        if i % 10== 0:\n",
    "            theta_list.append(theta.tolist())\n",
    "            cost = cost_function(hypothesis_function(X, theta), y)\n",
    "            cost_list.append(cost)\n",
    "    return theta, theta_list, cost_list\n",
    "# DO Linear regression with GD\n",
    "iterations = 70 # 학습횟수\n",
    "alpha = 0.00001 #학습률\n",
    "\n",
    "theta, theta_list, cost_list = gradient_descent(X, y, w, alpha, iterations)\n",
    "cost = cost_function(hypothesis_function(X, theta), y)\n",
    "\n",
    "print(\"theta:\", theta)\n",
    "print('cost:', cost_function(hypothesis_function(X, theta), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = np.array(theta_list)\n",
    "cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_step= np.dot(X, theta_list.transpose())\n",
    "y_predict_step\n",
    "plt.plot(raw_X,y,\"o\", alpha=0.3)\n",
    "for i in range (0,len(cost_list)):\n",
    "    plt.plot(raw_X,y_predict_step[:,i], label='Line %d'%i)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 진행 될 수록(line이 늘어날수록) 실제 데이터와 가까워 지는것을 확인 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_list)), cost_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cost function**의 값이 2번째 학습때부터 0으로 최소값으로 수렴하는 것을 볼 수 있습니다.\n",
    "\n",
    "## 7.4. Multivariate Linear Regression\n",
    "한개 이상의 feature로 구성된 데이터를 분석할때 사용하는 **Multivariate Linear Regression** 을 구현하는 방법에 대해 공부합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "def gen_data(numPoints, bias, variance):\n",
    "    x = np.zeros(shape=(numPoints, 3))\n",
    "    y = np.zeros(shape=numPoints)\n",
    "    # basically a straight line\n",
    "    for i in range(0, numPoints):\n",
    "        # bias feature\n",
    "        x[i][0] = random.uniform(0, 1) * variance + i\n",
    "        x[i][1] = random.uniform(0, 1) * variance + i\n",
    "        x[i][2] = 1\n",
    "        # our target variable\n",
    "        y[i] = (i+bias) + random.uniform(0, 1) * variance + 500\n",
    "    return x, y\n",
    "\n",
    "# gen 100 points with a bias of 25 and 10 variance as a bit of noise\n",
    "x, y = gen_data(100, 25, 10)\n",
    "\n",
    "plt.plot(x[:, 0:1], \"ro\")\n",
    "plt.plot(y, \"bo\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0], x[:,1], y)\n",
    "\n",
    "ax.set_xlabel('X0 Label')\n",
    "ax.set_ylabel('X1 Label')\n",
    "ax.set_zlabel('Y Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, theta):\n",
    "    '''\n",
    "    Comput cost for linear regression\n",
    "    '''\n",
    "    #Number of training samples\n",
    "    m = y.size\n",
    "    predictions = x.dot(theta)\n",
    "    sqErrors = (predictions - y)\n",
    "\n",
    "    J = (1.0 / (2 * m)) * sqErrors.T.dot(sqErrors)\n",
    "    return J\n",
    "\n",
    "def minimize_gradient(x, y, theta, iterations=100000, alpha=0.01):\n",
    "    m = y.size\n",
    "    cost_history = []\n",
    "    theta_history = []\n",
    "    \n",
    "    for _ in range(iterations):        \n",
    "        predictions = x.dot(theta)\n",
    "        \n",
    "        for i in range(theta.size):\n",
    "            partial_marginal = x[:, i]\n",
    "            errors_xi = (predictions - y) * partial_marginal\n",
    "            theta[i] = theta[i] - alpha * (1.0 / m) * errors_xi.sum()\n",
    "        \n",
    "        if _ % 1000 == 0:\n",
    "            theta_history.append(theta)\n",
    "            cost_history.append(compute_cost(x, y, theta))\n",
    "\n",
    "    return theta, np.array(cost_history), np.array(theta_history)\n",
    "\n",
    "theta_initial = np.ones(3)\n",
    "theta, cost_history, theta_history = minimize_gradient(\n",
    "        x, y,theta_initial, 300000, 0.0001)\n",
    "print(\"theta\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-fdf1953003bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mregr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn import  linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x[:,:2], y)\n",
    "\n",
    "# # The coefficients\n",
    "print('Coefficients: ', regr.coef_)\n",
    "print('intercept: ', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(theta, x[10]))\n",
    "print(regr.predict(x[10,:2].reshape(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(theta_history[:,0],theta_history[:,1], cost_history, zdir=\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100번째 학습이후 **cost function**이 수렴하는것을 알 수 있습니다.\n",
    "\n",
    "## 7.5. 성능 측정법(Performance measure)\n",
    "\n",
    "*만들어진 모델의 평가는 어떻게 할 것인가?* 를 판단하기 위해서는 평가할수있는 수치가 필요합니다. 이제 우리가 만든 모델을 평가하기 위해서 사용되는 **measure** 에 대한 개념과, `scikit-learn` 을 사용하여 어떻게 코드를 작성하는 지에 대해 공부합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error(MAE)\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "median_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error(RMSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R squared\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Stochastic Gradient Descent\n",
    "**Stochastic Gradient Descent(이하,SGD)** 알고리즘에 대한 개념과 GD와의 장단점이 무엇인지에 대해 비교해보겠습니다. 그리고 모델을 학습시킬 경우에 필수적인 **hyper parameter** 중 **epoch**과 **batch size** 에 대해서 함께 공부합니다.\n",
    "\n",
    "### 7.6.1. Full-batch gradient descent\n",
    "일반적으로 GD = full-batch GD라고 가정합니다. 모든 데이터 셋으로 학습하고 다음과 같은 특징을 가집니다.\n",
    "\n",
    "- 업데이트 감소 -> 계산상 효율적(속도) 가능\n",
    "- 안정적인 Cost 함수 수렴\n",
    "- 지역 최적화 가능\n",
    "- 메모리 문제가 발생할 수 있음\n",
    "- 대규모 dataset à 모델/파라메터 업데이트가 느려짐\n",
    "\n",
    "### 7.6.2. SGD\n",
    "전체 데이터 셋에서 임의로 training sample을 뽑은 후 학습에 사용합니다. 특징은 다음과 같습니다.\n",
    "- 빈번한 업데이트 모델 성능 및 개선 속도 확인 가능\n",
    "- 일부 문제에 대해 더 빨리 수렴\n",
    "- 지역 최적화 회피가능\n",
    "- 대용량 데이터시 시간이 오래걸림\n",
    "- 더 이상 cost가 줄어들지 않는 시점의 발견이 어려움\n",
    "\n",
    "### 7.6.3. Mini-SGD\n",
    "일부 데이터 셋에서 한 번에 일정 데이터를 임의로 뽑아서 학습합니다.\n",
    "- SGD와 Batch GD를 혼합한 기법\n",
    "- 가장 일반적으로 많이 쓰이는 기법\n",
    "\n",
    "### 7.6.4. SGD implementation issues\n",
    "SGD를 실제로 구현했을 때 생기는 여러 이슈는 다음과 같습니다.  \n",
    "- Learning-rate decay\n",
    "    - 일정한 주기로 Learning rate을 감소시키는 방법\n",
    "    - 특정 epoch 마다 Learning rate를 감소\n",
    "    - Hyper-parameter 설정의 어려움\n",
    "- 종료조건 설정\n",
    "    - SGD과정에서 특정 값이하로 cost function이 줄어들지 않을 경우 GD를 멈추는 방법\n",
    "    - 성능이 좋아지지 않는/필요없는 연산을 방지함\n",
    "    - 종료조건을 설정 : tol > (loss - previous_loss)\n",
    "    - tol은 hyperparameter로 사람이 설정함\n",
    "\n",
    "### 7.6.5. Epoch and batch-size\n",
    "- 전체 데이터가 Training 데이터에 들어갈 때 카운팅\n",
    "- Full-batch를 n번 실행하면 n epoch\n",
    "- Batch-size 한번에 학습되는 데이터의 개수\n",
    "    - 총 5,120개의 Training data에 512 batch-size면 몇 번 학습을 해야 1 epoch이 되는가? = 10번\n",
    "\n",
    "#### 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 데이터 불러오기\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "X = df.values\n",
    "y = boston.target\n",
    "# feature scailing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X)\n",
    "X_scaled = std_scaler.transform(X)\n",
    "# 데이터 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "# SGDRegression 하기\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "lr_SGD = SGDRegressor()\n",
    "lr_SGD.fit(X_train, y_train)\n",
    "\n",
    "y_hat = lr_SGD.predict(X_test)\n",
    "y_true = y_test\n",
    "# 평가하기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_hat, y_true)\n",
    "rmse = np.sqrt((((y_hat - y_true)**2).sum() / len(y_true)))\n",
    "# 시각화하기\n",
    "plt.scatter(y_true, y_hat, s=10)\n",
    "plt.xlabel(\"Prices: $Y_i$\")\n",
    "plt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
    "plt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7. 오버피팅(Overfitting) 과 정규화(Regularization)\n",
    "오버피팅이란 모델이 학습데이터에 과다 최적화되어 오히려 새로운 데이터 예측에는 성능이 떨어지는 것을 말합니다.\n",
    "\n",
    "### Overfitting 을 방지하기 위한 기법\n",
    "1. 데이터셋을 나누자\n",
    "2. 더 많은 데이터를 활용한다\n",
    "3. Feature의 개수를 줄인다\n",
    "4. 적절히 Parameter를 선정한다\n",
    "5. Regularization\n",
    "\n",
    "위의 기법 중에서 1번과 5번을 구체적으로 알아 보겠습니다.\n",
    "\n",
    "### 7.7.1. 데이터셋을 나누는 법\n",
    "학습한 데이터로 다시 테스트를 할 경우, 오버피팅이 됩니다. 테스트용 데이터는 기존의 학습 데이터와 차이가 있기 때문이죠. 모델은 새로운 데이터가 처리가능하도록 **generalize**되어 있어야 합니다. 그래서 학습용 데이터와 테스트용 데이터를 분리하는 기법이 필요합니다. 이러한 기법을 **holdout method**이라고 합니다.\n",
    "- 학습용과 테스트용 데이터를 나누는 비율은 데이터의 크기에 따라 다르지만 일반적으로 Training Data 70%, Test Data 30%을 사용합니다.\n",
    "\n",
    "`sci-kit learn`에서는 데이터셋을 간단히 나눌수 있습니다. 아래 예제 코드를 확인하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2. Regularization 방법\n",
    "L1 Regularization과 L2 Regularization의 개념과, 두 Regularization의 차이점에 대하여 공부합니다.\n",
    "\n",
    "-  L1 regularization: 기존 Cost function L1(norm) penalty term을 추가, Lasso 라고 합니다.\n",
    "-  L2 regularization: 기존 Cost function L2(norm) penalty term을 추가, Ridge 라고 합니다.\n",
    "\n",
    "#### 차이점 비교\n",
    "\n",
    "L1(Lasso) | L2(Ridge)\n",
    "---|---\n",
    "Unstable solution | Stable solution\n",
    "Always on solution | Only one solution\n",
    "Sparse solution  | Non-sparse solution\n",
    "Feature selection |  \n",
    "\n",
    "\n",
    "#### 예시 코드\n",
    " `scikit-learn` 에서 **linear** 모델 중 **SGD Regressor** 와 **Ridge, Lasso** 를 실제로 코드를 작성하고 실행하며 각각의 모델이 서로 어떤 특징을 가지는지에 대해 공부합니다. `scikit-learn`의 각 모델을 실행할 때 지정해야 하는 파라미터에 대한 설명도 함께 진행하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with Ridge & Lasso regression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "ridge = Ridge(fit_intercept=True, alpha=0.5)\n",
    "ridge.fit(X_train,y_train)\n",
    "#lasso = Lasso(fit_intercept=True, alpha=0.5)\n",
    "y_hat = ridge.predict(X_test)\n",
    "y_true = y_test\n",
    "mse = mean_squared_error(y_hat, y_true)\n",
    "rmse = np.sqrt((((y_hat - y_true)**2).sum() / len(y_true)))\n",
    "# rmse, mse\n",
    "plt.scatter(y_true, y_hat, s=10)\n",
    "plt.xlabel(\"Prices: $Y_i$\")\n",
    "plt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
    "plt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print('Ridge Regression')\n",
    "print('alpha\\t RMSE_train\\t RMSE_10cv\\n')\n",
    "alpha = np.linspace(.01,100,10)\n",
    "t_rmse = np.array([])\n",
    "cv_rmse = np.array([])\n",
    "\n",
    "for a in alpha:\n",
    "    ridge = Ridge(fit_intercept=True, alpha=a)\n",
    "    \n",
    "    # computing the RMSE on training data\n",
    "    ridge.fit(X_train,y_train)\n",
    "    p = ridge.predict(X_test)\n",
    "    err = p-y_test\n",
    "    total_error = np.dot(err,err)\n",
    "    rmse_train = np.sqrt(total_error/len(p))\n",
    "\n",
    "    # computing RMSE using 10-fold cross validation\n",
    "    kf = KFold(10)\n",
    "    xval_err = 0\n",
    "    for train, test in kf.split(X):\n",
    "        ridge.fit(X[train], y[train])\n",
    "        p = ridge.predict(X[test])\n",
    "        err = p - y[test]\n",
    "        xval_err += np.dot(err,err)\n",
    "    rmse_10cv = np.sqrt(xval_err/len(X))\n",
    "    \n",
    "    t_rmse = np.append(t_rmse, [rmse_train])\n",
    "    cv_rmse = np.append(cv_rmse, [rmse_10cv])\n",
    "    print('{:.3f}\\t {:.4f}\\t\\t {:.4f}'.format(a,rmse_train,rmse_10cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha, t_rmse, label='RMSE-Train')\n",
    "plt.plot(alpha, cv_rmse, label='RMSE_XVal')\n",
    "plt.legend( ('RMSE-Train', 'RMSE_XVal') )\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.3\n",
    "for name,met in [\n",
    "        ('lasso', Lasso(fit_intercept=True, alpha=a)),\n",
    "        ('ridge', Ridge(fit_intercept=True, alpha=a)),\n",
    "        ]:\n",
    "    met.fit(X_train,y_train)\n",
    "    # p = np.array([met.predict(xi) for xi in x])\n",
    "    p = met.predict(X_test)\n",
    "    e = p - y_test\n",
    "    total_error = np.dot(e,e)\n",
    "    rmse_train = np.sqrt(total_error/len(p))\n",
    "\n",
    "    kf = KFold(10)\n",
    "    err = 0\n",
    "    for train,test in kf.split(X):\n",
    "        met.fit(X[train],y[train])\n",
    "        p = met.predict(X[test])\n",
    "        e = p-y[test]\n",
    "        err += np.dot(e,e)\n",
    "\n",
    "    rmse_10cv = np.sqrt(err/len(X))\n",
    "    print('Method: %s' %name)\n",
    "    print('RMSE on training: %.4f' %rmse_train)\n",
    "    print('RMSE on 10-fold CV: %.4f' %rmse_10cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8. 다항회귀(Polynomial Regression)\n",
    "\n",
    "**Regression** 모델 중 X, Y 의 관계가 곡선 형태(비선형)일 경우에 사용할 수 있는 **Polynomial Regression**에 대한 개념과, `scikit-learn`을 활용해서 **polynomial regression** 을 수행하는 방법에 대해서 공부합니다.\n",
    "\n",
    "### 7.8.1. Polynomial Features\n",
    "1차 방정식을 고차다항식으로 변경하는 기법,`sklearn.preprocessing.PolynomialFeatures` 사용합니다. 다음과 같은 상황에서 사용됩니다. \n",
    "- 한개 변수가 Y값과 비선형적인 관계가 있다고 의심될 때\n",
    "- 주기적인 패턴을 보이는 Series 데이터\n",
    "- 모델 자체가 복잡해지면 해결가능한 부분이 많을때\n",
    "    - SVM, Tree-based models\n",
    "\n",
    "### 7.8.2. 최적화 하는 방법\n",
    "1. RMSE의 최소값을 찾는다\n",
    "2. Ridge, Lasso, LR 모두 다 사용해보기\n",
    "3. Degree 를 10 ~ 50까지 사용해보기\n",
    "\n",
    "###  예시 코드\n",
    "아래 코드로 자세히 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create matrix and vectors\n",
    "X = [[0.44, 0.68], [0.99, 0.23]]\n",
    "y = [109.85, 155.72]\n",
    "X_test = [0.49, 0.18]\n",
    "\n",
    "# PolynomialFeatures (prepreprocessing)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_ = poly.fit_transform(X)\n",
    "X_test_ = poly.fit_transform(X_test)\n",
    "\n",
    "# Instantiate\n",
    "lg = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "lg.fit(X_, y)\n",
    "\n",
    "# Obtain coefficients\n",
    "lg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "lg.predict(X_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9. Performance measure techniques\n",
    "머신러닝의 **Performance**를 측정하는 여러가지 기법에 대한 개념과, `scikit-learn` 으로 실행시키는 방법에 대해서 공부합니다.\n",
    "\n",
    "\n",
    "### 교차 검증(cross validation)이란? \n",
    "일반적으로 테스트 데이터가 별도로 존재하는 경우가 많지 않기 때문에 보통은 원래 학습용으로 확보한 데이터 중 일부를 떼어내어 테스트 데이터로 사용하는 경우가 많습니다. 그런데 데이터를 어떻게 나누냐에 따라 성능이 조금씩 달라질 수 있습니다. 따라서 여러가지 서로 다른 학습과 테스트 데이터를 사용해 여러번의 성능을 측정해 **평균 성능(mean performance)** 과 **성능 분산(performance variance)** 을 구하는 방법을 **교차 검증(cross validation)** 이라고 합니다.  \n",
    "\n",
    "`scikit-Learn` 의 `model_selection`기능은 교차 검증을 위해 전체 데이터 셋에서 학습용 데이터와 테스트용 데이터를 분리해 내는 여러가지 방법을 제공합니다. \n",
    "       \n",
    "### 7.9.1. K-fold cross validation\n",
    "- 학습 데이터를 K번 나눠서 Test와 Train을 실시 à Test의 평균값을 사용\n",
    "- 모델의 Parameter 튜닝, 간단한 모델의 최종 성능 측정 등 사용\n",
    "- `cross_val_score` 함수로, 한번에 해결 가능\n",
    "- `sklearn`은 이 후 작업의 통일성을 위해 **MSE**를 음수로 변환\n",
    "\n",
    "### 7.9.2. Leave One Out (LOO)\n",
    "하나의 sample만을 test set으로 남긴다.\n",
    "\n",
    "### 7.9.3. Validation set for parameter turning\n",
    "- Validation set의 많은 이유중 하나가 Hyper parameter turning\n",
    "- Number of iterations (SGD), Number of branch (Tree-based) etc.\n",
    "- Validation set의 성능으로 최적의 parameter를 찾음\n",
    "- Validation set 결과와 Training set 결과의 차이가 벌어지면 overfitting \n",
    "\n",
    "### 7.9.4. 기타등등\n",
    "- LeavePOut – 한번에 P개를 뽑음 (Not LOO for one data)\n",
    "- ShuffleSplit – 독립적인(중복되는) 데이터 Sampling\n",
    "- StratifiedKFold – Y 값 비율에 따라 뽑음\n",
    "- RepatedKFold – 중복이 포함된 K-Fold 생성\n",
    "- GroupKFold – 그룹별로 데이터를 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN - \", len(train_index))\n",
    "    print(\"TEST - \",  len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np \n",
    "\n",
    "lasso_regressor = Lasso(warm_start=False)\n",
    "ridge_regressor = Ridge()\n",
    "\n",
    "lasso_scores = cross_val_score(lasso_regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "ridge_scores= cross_val_score(ridge_regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(lasso_scores), np.mean(ridge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "test = [1, 2, 3, 4]\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(test):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 로지스틱 회귀(Logistic Regression)\n",
    "> 로지스틱 회귀(logistic regression)는 독립 변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는데 사용되는 통계 기법이다. -- wikipedia  \n",
    "\n",
    "분류 문제를 다루는 **Logistic Regression** 에 대해 공부합니다. 기존 접근의 문제점들은 다음과 같습니다.\n",
    "- 1이상 또는 0이하의 수들이 나오는 걸 어떻게 해석해야 할까?\n",
    "- 1 또는 0으로 정확히 표현 가능한가?\n",
    "- 변수가 Y에 영향을 주는정도가 비례하는가?\n",
    "- 확률로 발생할 사건의 가능성을 표현할 수 있는가?\n",
    "\n",
    "그에 대한 해법은 확률로 표현하는것입니다.\n",
    "\n",
    "## 9.1. Sigmoid function\n",
    "분류(Classification) 문제에서 분류되는 가능성을 확률로 표현하는 Sigmoid function 에 대해 공부합니다.\n",
    "\n",
    "### 9.1.1. Odds Ratio\n",
    "해당 사건이 일어날 확률과 일어나지 않을 확률의 비율\n",
    "- 일어날 확률 P(X)\n",
    "- 일어나지 않을 확률 1 - P(X)\n",
    "수식으로 나타내면 다음과 같습니다.  \n",
    "\n",
    "$$\\frac{P(X)}{1 - P(X)}$$\n",
    "\n",
    "### 9.1.2. Logit function\n",
    "X의 값이 주어졌을 때 y의 확률을 이용한 **log odds**. 수식은 다음과 같습니다.\n",
    "\n",
    "$$logit(P) =\\ln\\left(\\frac{P(X)}{1 - P(X)}\\right)$$\n",
    "\n",
    "### 9.1.3. Sigmoid(=Logistic) Function\n",
    "Logit 함수의 역함수로 z에 관한 확률을 산출합니다. 미분가능한 연속구간으로 변환되고, S 모양이라 하여 **sigmoid function**으로 부릅니다. 수식은 아래와 같습니다.\n",
    "\n",
    "$$ y = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "## 9.2. Cost function\n",
    "**Logistic Regression** 에서 사용하는 **Cost Function** 에 대해 알아봅니다\n",
    "\n",
    "\n",
    "## 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/binary.csv')\n",
    "# rename the 'rank' column because there is also a DataFrame method called 'rank'\n",
    "df.columns = [\"admit\", \"gre\", \"gpa\", \"prestige\"]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn  import  preprocessing  # Min-Max Standardzation\n",
    "\n",
    "y_data = df[\"admit\"].values.reshape(-1,1)\n",
    "x_data = df.ix[:,1:].values\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_data = min_max_scaler.fit_transform(x_data)\n",
    "\n",
    "x_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
    "\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=True)\n",
    "logreg.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도가 72% 정도로 만족스럽지는 않네요. \n",
    "\n",
    "# 10. 수강후기\n",
    "\n",
    "최성철 교수님께서 설명을 쉽게 해주셔서 이해하기 쉬웠던것 같습니다. 방대한 분량을 5일안에 강의하시다 보니, 뒷부분이 좀 너무 빠르게 지나간것 같아서 아쉽기는 합니다. 간만에 한국어로 강의를 듣다보니 집중해서 듣게 되고, 갑자기 의욕이 생깁니다. **Kaggle**에 들어가서 좀 더 배우도록 하겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nikola": {
   "category": "python",
   "date": "2018-05-28 07:27:02 UTC+09:00",
   "description": "edwith lecture about machine learning",
   "link": "",
   "slug": "edwith-machinelearning",
   "tags": [
    "Python",
    "machine learning",
    "edwith",
    "lecture note"
   ],
   "title": "edwith 머신러닝 강의 노트정리",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
